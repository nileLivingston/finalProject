{\rtf1\ansi\ansicpg1252\cocoartf1265
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww33400\viewh19520\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs54 \cf0 Learning of a Competitive Strategy in Scheisskopf\
\

\fs42 Scheisskopf is a card game intended for 2 or more players. As can be expected from the reasonably complicated rules of the game, competitive strategies are nontrivial and require attention to nonlocal constraints. This matter is complicated further by the fact that each player possesses imperfect information about the location of all cards in the deck. Our goal is to develop an agent that can learn to play competitively against a human opponent in a two-player version of the game.\
\
Rules of Scheisskopf:
\fs24 \
\pard\pardeftab720
\cf0 \

\b\fs34 	Dealing and pre-game:
\b0 \'a0Each player is dealt three cards face down, three cards face up, and a three card hand. The remaining cards form the deck. In the pre-game round, the players are allowed to swap cards in their hand with their up cards. The first player to finish swapping may then play a card, upon which all players must cease swapping and normal turn-taking begins.\
	
\b Taking your turn:
\b0 	A normal turn is as follows: the card(s) most recently played form the top of the pile. Let the card on top of the pile be of rank X. For most X (4, 5, 6, 8, 9, J, Q, K, A), the player whose turn it is must play at least one card from his/her hand of rank Y >= X. The exception to this rule is X=7, upon which the player must play card(s) of rank Y <= 7. Wild cards may be played on cards of any rank X. At the end of a turn, the player must draw exactly as many cards from the deck as needed to have 3 in their hand (if the player already has 3 or more cards in his/her hand, he/she does not draw). If the player draws card(s) of rank equal to the rank he/she just played, he/she may quickly play these before the opponent plays. If the player cannot play any cards, he/she must pick up the pile into his/her hand and his/her turn is over. On the other hand, if the player can play, he/she must; one may not elect to pick up the pile. A player\'92s up cards are unplayable until both his/her hand and the deck are empty. Similarly, a player\'92s bottom cards are unplayable until both his/her hand and up cards are exhausted. Down cards must be played one at a time and without looking at the card before playing it. Multiple up cards of the same rank may be played simultaneously. If, at any point, all four cards of a rank are played, the pile is discarded and the player who completed the set is allowed to play again.\
	
\b Endgame:\'a0
\b0 The goal of each player is to be the first to play all of his/her cards. This means exhausting the deck, one\'92s hand, one\'92s up cards, and successfully playing all of one\'92s down cards (i.e. if a down card is played that is not playable given the current up card, the player must pick up the deck into his/her previously empty hand).\
	
\b Card ordering:\'a0
\b0 The rank ordering of the non-wild cards is as follows: 4 < 5 < 6 < 7< 8 < 9 < J < Q < K < A.\

\b 	Wilds:\'a0
\b0 2\'92s, 3\'92s, and 10\'92s are wild. 2\'92s do nothing special, 3\'92s force the opponent to pick up the pile into his/her hand (the 3 played is discarded rather than picked up by the opponent), and 10\'92s discard the entire pile. Both 3\'92s and 10\'92s force the opponent to skip their turn.
\fs42 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \
Challenges:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\fs38 \cf0 	Imperfect information: At any given point in the game, there are factors that are unknown to each player in the game. The contents and ordering of the deck, the unobserved contents of the opponents hand, and each player\'92s down cards, are all unknown. In practice, this uncertainty extends to the contents of the discard pile and possibly most of the contents of the opponents hand due to the limitations of memory for human players. A computer agent thus has the advantage of perfect memory of what it has seen; it may know the contents of the discard pile, as well as perhaps some of the opponent\'92s hand. Where the computer agent is less naturally talented is in assessing risk (e.g. choosing between playing a non-wild or a 10, unsure of whether or not the opponent has a 3). Making potentially risky choices like this in situations with imperfect information requires a reasonable system of inference, whether explicit (e.g. a probabilistic inference system) or implicit (policies acquired through Q-learning).
\fs42 \
\
Schedule:\

\fs36 	Phase I (week 1): During this phase of the project, we intend to fully implement the game environment. This will involve constructing a simple, functional GUI for the human player as well as implementing a default agent that will select actions randomly. Completion of this phase of the project will allow a human player to play an entire game of realistic Scheisskopf against a randomly-acting opponent. We intend to use the Tkinter package (a general purpose Python GUI) for implementing our program\'92s GUI. \
\
	Phase II (week 2): During this phase, we intend to implement and test both a Q-learner and a reflex agent. The reflex agent will rank actions based on a series of well-defined, reasonably simple heuristics that match our intuitions as amateur-expert players of Scheisskopf. The Q-learner will be trained against this reflex agent. Because of the massive state space of most card games (Scheisskopf included), we will have to featurize our states in order to pose a tractable Q-learning problem. Ideas for helpful features include:\
	-# of cards in opponent\'92s hand\
	-# of cards in agent\'92s hand\
	-# of wilds in opponent\'92s hand\
	-# of wilds in agent\'92s hand\
	-Size of pile\
\
	Phase III (week 3): During this phase, we hope to improve upon our agent, possibly by exploring alternative learning algorithms, fine tuning the featurization of states, or by incorporating certain heuristics into the learning agent\'92s strategy. It is during this time that we hope to perform assessment of various strategies to show that, in fact, our finalized agent plays respectably.
\fs42 \
\
Assessment:\
\
\
\
}